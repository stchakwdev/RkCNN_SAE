# Phase 2: GPT-2 SAE Configuration
# Compare baseline SAE vs RkCNN-initialized SAE on GPT-2 activations

# IMPORTANT: Only run Phase 2 after Phase 1 passes!

# Model parameters
model_name: "gpt2"      # GPT-2 model (gpt2, gpt2-medium, gpt2-large)
layer: 6                # Layer to extract activations from
hook_point: "mlp_out"   # Hook point (mlp_out, resid_post)

# Data parameters
max_tokens: 1000000     # Maximum tokens to cache (1M)
use_synthetic: false    # Use real GPT-2 (requires transformer-lens + GPU)

# SAE parameters
expansion_factor: 8     # n_latents = d_model * expansion_factor
l1_coefficient: 0.001   # L1 sparsity penalty
normalize_decoder: true

# Training parameters
n_train_steps: 10000    # Number of training steps
batch_size: 256
lr: 0.001

# RkCNN parameters
rkcnn_fraction: 0.5     # Fraction of latents to init with RkCNN (50%)
rkcnn_n_subsets: 600    # Number of random subsets for RkCNN
rkcnn_score_method: "kurtosis"  # Scoring method for subsets
# rkcnn_subset_size defaults to sqrt(d_model) = 28 for GPT-2

# Execution
seed: 42
device: "cuda"          # Requires GPU for efficient training
verbose: true

# Output and checkpointing
output_dir: "./results/phase2"
checkpoint_dir: "./checkpoints"
checkpoint_every: 1000  # Checkpoint every N steps

# Success criteria targets
target_dead_latent_rate: 0.2  # < 20% dead latents
target_l0_tolerance: 1.1      # L0 can be up to 10% worse than baseline
target_recon_tolerance: 1.1   # Reconstruction can be up to 10% worse
